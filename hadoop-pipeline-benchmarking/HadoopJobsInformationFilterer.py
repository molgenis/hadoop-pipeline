#!/user/bin/env python3
"""
Name: HadoopJobsInformationMerger.py
Usage:
	HadoopJobsInformationMerger.py <inputdir> <outputdir>

Description:
	Reads in all files generated by HadoopJobInformationRetriever.py and merges the data
	of multiple jobs into more usable files. Note that for this to correctly work,
	every time HadoopJobInformationRetriever.py is executed, the same output directory
	needs to be given.
	
	This script will generate 3 output files into the output directory:
	- A csv file containing several job counters (contains a header line)
	- A csv file containing the CPU_MILLISECONDS of all map tasks, 1 line per job *
	- A csv file containing the CPU_MILLISECONDS of all reduce tasks, 1 line per job *
	
	* = The first column contains the job id, while all other columns contain values.
"""

import sys
import os
from types import GeneratorType 
import re
import json
import objectpath # installation: see http://objectpath.org/
# install pypz3 for warning from objectpath to disappear: pip3 install pytz

def main():
	"""
	Name:
		main
		
	Info:
		Runs the main application.
	"""
	
	# Generate required variables from user-input.
	inputDir = sys.argv[1].rstrip('/') + '/'
	outputDir = sys.argv[2].rstrip('/') + '/'
	
	# Generates the output directory.
	createOutputDirectory(outputDir)
	
	# Stores the output to be written to files.
	jobData = {}
	mapTaskData = {}
	reduceTaskData = {}
	
	# Goes through all files in the directory.
	for dirPath, subDirs, files in os.walk(inputDir):
		for fileName in files:
			# Empty dict for temporarily storing data from a single file.
			dataToStore = {}
			
			# Retrieves the job id based on the current path.
			jobId = os.path.basename(dirPath)
			
			# Processes a job info file.
			if fileName == 'job_info.json':
				processJobInfo(generateTree(dirPath, fileName), dataToStore)
				storeJobData(jobData, dataToStore)
			
			# Process a job counters file.
			elif fileName == 'job_counters.json':
				processJobCounters(generateTree(dirPath, fileName), dataToStore)
				storeJobData(jobData, dataToStore)
			
			# Process a task file.
			elif fileName.startswith('task_'):
				# Process a task info file.
				if fileName.endswith('_info.json'):
					processTaskInfo(generateTree(dirPath, fileName), dataToStore)
					if isMapperTask(dataToStore):
						storeTaskData(mapTaskData, dataToStore)
					else:
						storeTaskData(reduceTaskData, dataToStore)
				
				# Process a task counters file.
				elif fileName.endswith('_counters.json'):
					taskCountersTree = generateTree(dirPath, fileName)
					processTaskCounters(taskCountersTree, dataToStore)
					
					# Process a task map counters file.
					if isMapperTask(dataToStore):
						processMapTaskCounters(taskCountersTree, dataToStore)
						storeTaskData(mapTaskData, dataToStore)
					# Process a task reduce counters file.
					else:
						processReduceTaskCounters(taskCountersTree, dataToStore)
						storeTaskData(reduceTaskData, dataToStore)
	
	# Writes the data to files.
	writeJobDataToFile(outputDir, 'jobs.csv', jobData)
	writeTaskDataToFile(outputDir, 'tasks_map.csv', mapTaskData)
	writeTaskDataToFile(outputDir, 'tasks_reduce.csv', reduceTaskData)

def processJobInfo(tree, dataToStore):
	"""
	Name:
		processJobInfo
		
	Info:
		Retrieves all fields of interest from a job info file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Used for merging these results with results retrieved from other job files of the same job.
	dataToStore['JOB_ID'] = retrieveFromTree(tree,"$.job.id")
	
	# Retrieves values of interest.
	startTime = retrieveFromTree(tree,"$.job.startTime")
	finishTime = retrieveFromTree(tree,"$.job.finishTime")
	dataToStore['ELAPSED_TIME'] = finishTime - startTime
	
	dataToStore['AVERAGE_MAP_TIME'] = retrieveFromTree(tree,"$.job.avgMapTime")
	dataToStore['AVERAGE_REDUCE_TIME'] = retrieveFromTree(tree,"$.job.avgReduceTime")
	dataToStore['AVERAGE_SHUFFLE_TIME'] = retrieveFromTree(tree,"$.job.avgShuffleTime")
	dataToStore['AVERAGE_MERGE_TIME'] = retrieveFromTree(tree,"$.job.avgMergeTime")

def processJobCounters(tree, dataToStore):
	"""
	Name:
		processJobCounters
		
	Info:
		Retrieves all fields of interest from a job counters file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Used for merging these results with results retrieved from other job files of the same job.
	dataToStore['JOB_ID'] = retrieveFromTree(tree,"$.jobCounters.id")
	
	# Retrieves values of interest.
	# The [0] after the counterGroupName selector in taskCounterGroup can in theory return multiple groups
	# having that taskCounterGroup, though the json objects should only contain that group name once.
	# The final output can in theory contain multiple values with the defined counter name, but this should
	# not be the case and therefore after the list cast only the first element is taken.
	# See also the retrieveFromTree(tree, path) method.
	dataToStore['NUM_KILLED_MAPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'NUM_KILLED_MAPS'].totalCounterValue")
	dataToStore['NUM_KILLED_REDUCES'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'NUM_KILLED_REDUCES'].totalCounterValue")
	dataToStore['TOTAL_LAUNCHED_MAPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'TOTAL_LAUNCHED_MAPS'].totalCounterValue")
	dataToStore['TOTAL_LAUNCHED_REDUCES'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'TOTAL_LAUNCHED_REDUCES'].totalCounterValue")
	dataToStore['OTHER_LOCAL_MAPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'OTHER_LOCAL_MAPS'].totalCounterValue")
	dataToStore['DATA_LOCAL_MAPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'DATA_LOCAL_MAPS'].totalCounterValue")
	dataToStore['RACK_LOCAL_MAPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.JobCounter'][0].counter[@.name is 'RACK_LOCAL_MAPS'].totalCounterValue")
	
	dataToStore['MAP_OUTPUT_RECORDS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'MAP_OUTPUT_RECORDS'].totalCounterValue")
	dataToStore['REDUCE_INPUT_GROUPS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'REDUCE_INPUT_GROUPS'].totalCounterValue")
	dataToStore['REDUCE_INPUT_RECORDS'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'REDUCE_INPUT_RECORDS'].totalCounterValue")
	dataToStore['CPU_MILLISECONDS_TOTAL'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'CPU_MILLISECONDS'].totalCounterValue")
	dataToStore['CPU_MILLISECONDS_MAP'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'CPU_MILLISECONDS'].mapCounterValue")
	dataToStore['CPU_MILLISECONDS_REDUCE'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'CPU_MILLISECONDS'].reduceCounterValue")
	
	dataToStore['BOTH_UNMAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'BOTH_UNMAPPED'].totalCounterValue")
	dataToStore['BOTH_MAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'BOTH_MAPPED'].totalCounterValue")
	dataToStore['BOTH_MULTIMAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'BOTH_MULTIMAPPED'].totalCounterValue")
	dataToStore['BOTH_MULTIMAPPED_SUPPLEMENTARY_ONLY'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'BOTH_MULTIMAPPED_SUPPLEMENTARY_ONLY'].totalCounterValue")
	dataToStore['ONE_UNMAPPED_ONE_MAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_UNMAPPED_ONE_MAPPED'].totalCounterValue")
	dataToStore['ONE_UNMAPPED_ONE_MULTIMAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_UNMAPPED_ONE_MULTIMAPPED'].totalCounterValue")
	dataToStore['ONE_UNMAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_UNMAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'].totalCounterValue")
	dataToStore['ONE_MAPPED_ONE_MULTIMAPPED'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_MAPPED_ONE_MULTIMAPPED'].totalCounterValue")
	dataToStore['ONE_MAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_MAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'].totalCounterValue")
	dataToStore['ONE_MULTIMAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'ONE_MULTIMAPPED_ONE_MULTIMAPPED_SUPPLEMENTARY_ONLY'].totalCounterValue")
	dataToStore['INVALID'] = retrieveFromTree(tree,"$.jobCounters.counterGroup[@.counterGroupName is 'org.molgenis.hadoop.pipeline.application.sequences.AlignedReadPairType'][0].counter[@.name is 'INVALID'].totalCounterValue")

def processTaskInfo(tree, dataToStore):
	"""
	Name:
		processTaskInfo
		
	Info:
		Retrieves all fields of interest from a task info file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Used for merging these results with results retrieved from other task files of the same task.
	dataToStore['TASK_ID'] = retrieveFromTree(tree,"$.task.id")
	
	# Retrieves values of interest.
	dataToStore['ELAPSED_TIME'] = retrieveFromTree(tree,"$.task.elapsedTime")
	dataToStore['STATE'] = retrieveFromTree(tree,"$.task.state")

def processTaskCounters(tree, dataToStore):
	"""
	Name:
		processTaskCounters
		
	Info:
		Retrieves all fields of interest from a task counters file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Used for merging these results with results retrieved from other task files of the same task.
	dataToStore['TASK_ID'] = retrieveFromTree(tree,"$.jobTaskCounters.id")
	
	# Retrieves values of interest.
	# The [0] after the counterGroupName selector in taskCounterGroup can in theory return multiple groups
	# having that taskCounterGroup, though the json objects should only contain that group name once.
	# The final output can in theory contain multiple values with the defined counter name, but this should
	# not be the case and therefore after the list cast only the first element is taken.
	# See also the retrieveFromTree(tree, path) method.
	dataToStore['FILE_BYTES_READ'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.FileSystemCounter'][0].counter[@.name is 'FILE_BYTES_READ'].value")
	dataToStore['CPU_MILLISECONDS'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'CPU_MILLISECONDS'].value")

def processMapTaskCounters(tree, dataToStore):
	"""
	Name:
		processMapTaskCounters
		
	Info:
		Retrieves all map-specific fields of interest from a task counters file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Retrieves values of interest.
	# The [0] after the counterGroupName selector in taskCounterGroup can in theory return multiple groups
	# having that taskCounterGroup, though the json objects should only contain that group name once.
	# The final output can in theory contain multiple values with the defined counter name, but this should
	# not be the case and therefore after the list cast only the first element is taken.
	# See also the retrieveFromTree(tree, path) method.
	dataToStore['MAP_INPUT_RECORDS'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'MAP_INPUT_RECORDS'].value")
	dataToStore['MAP_OUTPUT_RECORDS'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'MAP_OUTPUT_RECORDS'].value")

def processReduceTaskCounters(tree, dataToStore):
	"""
	Name:
		processReduceTaskCounters
		
	Info:
		Retrieves all reduce-specific fields of interest from a task counters file and stores it in dataToStore.
	
	Input:
		tree - Tree: Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
			See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
			for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
		dataToStore - dict: In which the counters of interest should be stored.
	"""
	
	# Retrieves values of interest.
	# The [0] after the counterGroupName selector in taskCounterGroup can in theory return multiple groups
	# having that taskCounterGroup, though the json objects should only contain that group name once.
	# The final output can in theory contain multiple values with the defined counter name, but this should
	# not be the case and therefore after the list cast only the first element is taken.
	# See also the retrieveFromTree(tree, path) method.
	dataToStore['REDUCE_INPUT_GROUPS'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'REDUCE_INPUT_GROUPS'].value")
	dataToStore['REDUCE_INPUT_RECORDS'] = retrieveFromTree(tree,"$.jobTaskCounters.taskCounterGroup[@.counterGroupName is 'org.apache.hadoop.mapreduce.TaskCounter'][0].counter[@.name is 'REDUCE_INPUT_RECORDS'].value")

def retrieveFromTree(tree, path):
	"""
	Name:
		retrieveFromTree
		
	Info:
		Retrieves the first value from the given path from the tree. The final output can in theory contain
		multiple values with the defined counter name, but this should not be the case for what is retrieved
		in this script and therefore after the list cast only the first element is returned.
	
	Input:
		tree - objectpath.Tree: A tree to look in.
		path - String: The path to be used for looking inside the tree.
	
	Output:
		A String 'n.a.' if path returned no values (can be caused by an invalid path), the value itself if exactly
		1 value is found (which can be a String, int or alike) or a string containing the path combined with a
		message saying multiple values were retrieved if multiple values were found.
	"""
	
	values = tree.execute(path)
	
	# Converts generator types to lists and returns value (or certain if checks failed).
	if isinstance(values, GeneratorType):
		values = list(values)
		nValues = len(values)
		
		if nValues == 0:
			return 'NA'
		elif nValues > 1:
			raise Exception('Invalid json file. The following path returns multiple values:' + os.linesep + path)
		else:
			return values[0]
	
	# If output is an int, simply returns it
	elif isinstance(values, int):
		return values
	
	# If output is a String, simply returns it
	elif isinstance(values, str):
		return values
	
	# If type is something else, process as 'unknown' but do not exit application.
	else:
		raise Exception('Invalid json file. The following path returns a value of unexpected type "' + str(type(values)) + '":' + os.linesep + path)

def storeJobData(storedData, dataToStore):
	"""
	Name:
		storeJobData
		
	Info:
		Stores the job data defined by dataToStore inside storedData. If data with the same job id is already
		present, the data is merged together.
	
	Input:
		storedData - dict(jobId:dict()): Where the data should be stored in.
		dataToStore - dict(): The data that should be stored.
	"""
	
	jobId = dataToStore.get('JOB_ID')
	
	storedJobData = storedData.get(jobId)
	if storedJobData != None:
		storedJobData.update(dataToStore)
	else:
		storedData[jobId] = dataToStore

def storeTaskData(storedData, dataToStore):
	"""
	Name:
		storeTaskData
		
	Info:
		Stores the task data defined by dataToStore inside storedData. If data with the same job id is already
		present, it looks whether a task is already present with the same task id for that job and if so,
		merges the data together.
	
	Input:
		storedData - dict(jobId:list(dict())): Where the data should be stored in.
		dataToStore - dict(): The data that should be stored.
	"""
	
	taskId = dataToStore.get('TASK_ID') 
	jobId = getJobIdFromTaskId(taskId)
	
	tasks = storedData.get(jobId)
	if tasks != None:
		# Looks if task is already present and merges data if so.
		for task in tasks:
			if task.get('TASK_ID') == taskId:
				task.update(dataToStore)
				return # Makes sure it is not appended as well.
		
		# If task is not present yet, simply appends it.
		tasks.append(dataToStore)
	
	# If job is not present yet, creates a new key:value pair.
	else:
		storedData[jobId] = [dataToStore]

def readFileAsJson(fileName):
	"""
	Name:
		readFileAsJson
		
	Info:
		Reads in a file path as Json object.
	
	Input:
		fileName - String: Path to file that should be read.
	"""
	
	with open(fileName) as dataFile:
		return json.load(dataFile)

def generateTree(dirPath, fileName):
	"""
	Name:
		generateTree
		
	Info:
		Generates an objectpath tree from a json file using a directory path and filename.
	
	Input:
		dirPath - String: Path to a directory.
		fileName - String: Name of the json file in the dirPath.
	
	Output:
		Json structure generated by the Hadoop REST API and processed by objectpath.Tree().
		See also https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html
		for more information about the MapReduce History Server REST API's (Hadoop v2.6.0).
	"""
	
	fullPath = os.path.join(dirPath, fileName)
	return objectpath.Tree(readFileAsJson(fullPath))

def isMapperTask(dataToStore):
	"""
	Name:
		isMapperTask
		
	Info:
		Checks if a task is a mapper task based on the task id.
	
	Input:
		taskId - String: The task id.
	
	Output:
		boolean: True if task is a mapper task, otherwise False.
	"""
	
	if dataToStore.get('TASK_ID').split('_')[3] == 'm':
		return True
	else:
		return False

def getJobIdFromTaskId(taskId):
	"""
	Name:
		getJobIdFromTaskId
		
	Info:
		Retrieved the job id from a task id.
	
	Input:
		taskId - String: The task id.
	
	Output:
		String: The job id.
	"""
	
	# example: task_0123456789012_01234_m_000000 -> job_0123456789012_01234
	return 'job_' + '_'.join(taskId.split('_')[1:3])

def createOutputDirectory(directory):
	"""
	Name:
		createOutputDirectory
		
	Info:
		Checks if the path exists. If not, creates it (recursively).
	
	Input:
		directory - String: Path on OS.
	"""
	
	if not os.path.exists(directory):
		os.makedirs(directory)

def writeJobDataToFile(outputDir, fileName, jobData):
	"""
	Name:
		writeJobCountersToFile
		
	Info:
		Writes retrieved job information to a file.
	
	Input:
		outputDir - String: The output directory.
		fileName - String: The output file name to be used.
		countersData - dict(dict()): A dict containing a job id as
			key with as value a dict with counter names (keys) and accompanying
			values (values). If a TASK_ID is present, makes sure this is
			the second column in the output file.
	"""
	
	# Retrieves and sorts the counter names from which values were retrieved.
	counterNames = list(list(jobData.values())[0].keys())
	counterNames.sort()
	
	# Places the JOB_ID as first item in the list.
	counterNames.remove('JOB_ID')
	counterNames.insert(0, 'JOB_ID')
	
	# Open file to write to.
	f = open(outputDir + fileName, 'w')
	
	# Writes header to file.
	f.write(','.join(counterNames) + os.linesep)
	
	# Writes values to the file.
	for job in jobData.keys():
		values = []
		for name in counterNames:
			values.append(str(jobData.get(job).get(name)))
		f.write(','.join(values) + os.linesep)
	
	# Close file.
	f.close()

def writeTaskDataToFile(outputDir, fileName, taskData):
	"""
	Name:
		writeTaskCountersToFile
		
	Info:
		Writes retrieved task information to a file.
	
	Input:
		outputDir - String: The output directory.
		fileName - String: The output file name to be used.
		taskCountersData - dict(list(dict())): A dict containing a job id as
			key and as value a list with task dicts. Each task dict contains
			counter names as keys and accompanying values as values.
	"""
	
	# Retrieves and sorts the counter names from which values were retrieved.
	counterNames = list(list(taskData.values())[0][0].keys())
	counterNames.sort()
	
	# Places the TASK_ID as first item in the list.
	counterNames.remove('TASK_ID')
	counterNames.insert(0, 'TASK_ID')
	
	# Open file to write to.
	f = open(outputDir + fileName, 'w')
	
	# Writes header to file.
	f.write('JOB_ID,' + ','.join(counterNames) + os.linesep)
	
	for job in taskData.keys():
		for task in taskData.get(job):
			values = []
			for name in counterNames:
				values.append(str(task.get(name)))
			
			f.write(getJobIdFromTaskId(task.get('TASK_ID')) + ',' + ','.join(values) + os.linesep)
	
	# Close file.
	f.close()


# Only run main() automatically if this script is executed directly.
if __name__ == '__main__':
	main()
